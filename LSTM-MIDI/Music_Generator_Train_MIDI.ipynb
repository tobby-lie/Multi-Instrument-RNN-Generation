{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Music_Generator_Train_MIDI.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3xEQZ-cmYi3E","colab_type":"code","outputId":"58b36c48-eb10-40ee-e316-cf0eb879e088","executionInfo":{"status":"ok","timestamp":1573792141776,"user_tz":420,"elapsed":21096,"user":{"displayName":"Tobby Lie","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBETvj4lEXE5Zw_hp4pXOL58CMwxM-L-_9Mx63urA=s64","userId":"17464463282068902386"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["\"\"\" This module prepares midi file data and feeds it to the neural\n","    network for training \"\"\"\n","# set the matplotlib backend so figures can be saved in the background\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","\n","import glob\n","import pickle\n","import numpy\n","from music21 import converter, instrument, note, chord\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.layers import Activation\n","from keras.utils import np_utils\n","from keras.callbacks import ModelCheckpoint\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s-4U0GStcmNc","colab_type":"code","outputId":"ad8cb8c2-53a2-4971-add0-61efe6d13bcc","executionInfo":{"status":"ok","timestamp":1573792142825,"user_tz":420,"elapsed":22138,"user":{"displayName":"Tobby Lie","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBETvj4lEXE5Zw_hp4pXOL58CMwxM-L-_9Mx63urA=s64","userId":"17464463282068902386"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["%cd '/content/drive/My Drive/Colab_Notebooks_Music_Gen'\n","%ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab_Notebooks_Music_Gen\n","\u001b[0m\u001b[01;34mdata\u001b[0m/                             plot.png\n","\u001b[01;34mmidi_songs\u001b[0m/                       test_output.mid\n","Music_Generator_MIDI.ipynb        weights-improvement-01-4.4937-bigger.hdf5\n","Music_Generator_Train_MIDI.ipynb  weights-improvement-195-0.0123-bigger.hdf5\n","new_weights.hdf5                  weights-improvement-53-0.0766-bigger.hdf5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CNYCH__0F97K","colab_type":"code","colab":{}},"source":["def train_network():\n","    \"\"\" Train a Neural Network to generate music \"\"\"\n","    \n","    # get list of notes and normalOrder notation for chords\n","    notes = get_notes()\n","    \n","    # get amount of pitch names\n","    # set indicates no duplicates\n","    n_vocab = len(set(notes))\n","     \n","    # get input and output for network\n","    network_input, network_output = prepare_sequences(notes, n_vocab)\n","\n","    # create the model for training\n","    model = create_network(network_input, n_vocab)\n","\n","    H = train(model, network_input, network_output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjdjiJB2GAQN","colab_type":"code","colab":{}},"source":["def get_notes():\n","    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n","    \n","    # empty list to contain all notes\n","    notes = []\n","    \n","    # get all files in midi_songs directory in the form of\n","    # \"midi_songs/*.mid\"\n","    for file in glob.glob(\"midi_songs/*.mid\"):\n","      \n","        # from file get Score produced from parse function\n","        midi = converter.parse(file)\n","  \n","        # notify which file is being parsed\n","        print(\"Parsing %s\" % file)\n","\n","        notes_to_parse = None\n","\n","        try: # file has instrument parts\n","            # partition the midi file by instruments, return that list\n","            # into s2\n","            s2 = instrument.partitionByInstrument(midi)\n","            \n","            # parses first part of midi \n","            # recurse() will visit every element in the stream, \n","            # starting from the beginning, and if any of the \n","            # subelements are also Streams, they will visit \n","            # every element in that Stream.\n","            notes_to_parse = s2.parts[0].recurse() \n","            \n","        except: # file has notes in a flat structure\n","            notes_to_parse = midi.flat.notes\n","\n","        # loop through elements in notes_to_parse\n","        for element in notes_to_parse:\n","            # is element a note object?\n","            if isinstance(element, note.Note):\n","                # if so append the pitch (note) to the notes list\n","                notes.append(str(element.pitch))\n","            # is element a chord object?\n","            elif isinstance(element, chord.Chord):\n","                # if so append the chord to the notes list by joining\n","                # each element in normalOrder list of integer representation\n","                notes.append('.'.join(str(n) for n in element.normalOrder))\n","    \n","    # open 'data/notes' file for writing in binary format since we are\n","    # dealing with non text format\n","    with open('data/notes', 'wb') as filepath:\n","        # write notes in binary format to filepath\n","        pickle.dump(notes, filepath)\n","    # return notes list\n","    return notes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ehnQ567qGEaU","colab_type":"code","colab":{}},"source":["def prepare_sequences(notes, n_vocab):\n","    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n","    # define a sequence length, can be any length\n","    sequence_length = 100\n","\n","    # get all unique pitch names in sorted order\n","    # including notes and normalOrder chords\n","    pitchnames = sorted(set(item for item in notes))\n","\n","    # create a dictionary to map pitches to integers\n","    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n","  \n","    # create empty lists for input and output\n","    network_input = []\n","    network_output = []\n","\n","    # create input sequences and the corresponding outputs\n","    # go from 0 to len(notes) - sequence_length in increments of 1\n","    for i in range(0, len(notes) - sequence_length, 1):\n","        # sequence_in is a sequence of length 100 \n","        # going from i all the way to one before i + sequence length\n","        # in notes list\n","        sequence_in = notes[i:i + sequence_length]\n","        # sequence out is a single note from notes list\n","        # that is at position i + sequence length\n","        # right after the last note in sequence_in \n","        sequence_out = notes[i + sequence_length]\n","        \n","        # append all int representations of notes or chords from sequence_n\n","        network_input.append([note_to_int[char] for char in sequence_in])\n","        # append int representation of note or chord from sequence_out\n","        network_output.append(note_to_int[sequence_out])\n","\n","    # number of patterns is eq to length of network input\n","    n_patterns = len(network_input)\n","\n","    # reshape the input into a format compatible with LSTM layers\n","    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n","    # normalize input\n","    network_input = network_input / float(n_vocab)\n","\n","    # converts a class vector (integers) to binary class matrix.\n","    network_output = np_utils.to_categorical(network_output)\n","    \n","    return (network_input, network_output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-vo8qshGHhc","colab_type":"code","colab":{}},"source":["def create_network(network_input, n_vocab):\n","    \"\"\" create the structure of the neural network \"\"\"\n","    model = Sequential()\n","    model.add(LSTM(\n","        512,\n","        input_shape=(network_input.shape[1], network_input.shape[2]),\n","        return_sequences=True\n","    ))\n","    model.add(Dropout(0.3))\n","    model.add(LSTM(512, return_sequences=True))\n","    model.add(Dropout(0.3))\n","    model.add(LSTM(512))\n","    model.add(Dense(256))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(n_vocab))\n","    model.add(Activation('softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GtSd-oWtGKnN","colab_type":"code","colab":{}},"source":["def train(model, network_input, network_output):\n","    \"\"\" train the neural network \"\"\"\n","    nb_epochs = 200\n","    filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n","    checkpoint = ModelCheckpoint(\n","        filepath,\n","        monitor='loss',\n","        verbose=1,\n","        save_best_only=True,\n","        mode='min'\n","    )\n","    callbacks_list = [checkpoint]\n","\n","    H = model.fit(network_input, network_output, epochs=nb_epochs, batch_size=32, callbacks=callbacks_list)\n","    print(\"Loss\")\n","    print(H.history[\"loss\"])\n","    print(\"Accuracy\")\n","    print(H.history[\"acc\"])\n","\n","    # plot the training loss and accuracy\n","    plt.style.use(\"ggplot\")\n","    plt.figure()\n","    N = nb_epochs\n","    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n","    plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n","    plt.plot(H.history[\"acc\"])\n","    plt.title(\"Training Loss and Accuracy\")\n","    plt.xlabel(\"Epoch #\")\n","    plt.ylabel(\"Loss/Accuracy\")\n","    plt.legend(loc=\"upper left\")\n","    plt.savefig(\"plot.png\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5Zig9sRGMBw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"fc928858-fd69-48f2-8297-99fb2d6bdd7e"},"source":["if __name__ == '__main__':\n","    train_network()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Parsing midi_songs/0fithos.mid\n","Parsing midi_songs/8.mid\n","Parsing midi_songs/ahead_on_our_way_piano.mid\n","Parsing midi_songs/AT.mid\n","Parsing midi_songs/Fiend_Battle_(Piano).mid\n","Parsing midi_songs/DOS.mid\n","Parsing midi_songs/ff4-airship.mid\n","Parsing midi_songs/FFVII_BATTLE.mid\n","Parsing midi_songs/bcm.mid\n","Parsing midi_songs/ff7themep.mid\n","Parsing midi_songs/cosmo.mid\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/200\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","6188/6188 [==============================] - 85s 14ms/step - loss: 4.3765 - acc: 0.0477\n","\n","Epoch 00001: loss improved from inf to 4.37654, saving model to weights-improvement-01-4.3765-bigger.hdf5\n","Epoch 2/200\n","6188/6188 [==============================] - 77s 12ms/step - loss: 4.1352 - acc: 0.0687\n","\n","Epoch 00002: loss improved from 4.37654 to 4.13516, saving model to weights-improvement-02-4.1352-bigger.hdf5\n","Epoch 3/200\n","6188/6188 [==============================] - 76s 12ms/step - loss: 4.0734 - acc: 0.0763\n","\n","Epoch 00003: loss improved from 4.13516 to 4.07340, saving model to weights-improvement-03-4.0734-bigger.hdf5\n","Epoch 4/200\n","6188/6188 [==============================] - 76s 12ms/step - loss: 3.9899 - acc: 0.0970\n","\n","Epoch 00004: loss improved from 4.07340 to 3.98986, saving model to weights-improvement-04-3.9899-bigger.hdf5\n","Epoch 5/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 3.8636 - acc: 0.1052\n","\n","Epoch 00005: loss improved from 3.98986 to 3.86360, saving model to weights-improvement-05-3.8636-bigger.hdf5\n","Epoch 6/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 3.7158 - acc: 0.1383\n","\n","Epoch 00006: loss improved from 3.86360 to 3.71584, saving model to weights-improvement-06-3.7158-bigger.hdf5\n","Epoch 7/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 3.5272 - acc: 0.1681\n","\n","Epoch 00007: loss improved from 3.71584 to 3.52718, saving model to weights-improvement-07-3.5272-bigger.hdf5\n","Epoch 8/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 3.3206 - acc: 0.1968\n","\n","Epoch 00008: loss improved from 3.52718 to 3.32056, saving model to weights-improvement-08-3.3206-bigger.hdf5\n","Epoch 9/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 3.1127 - acc: 0.2274\n","\n","Epoch 00009: loss improved from 3.32056 to 3.11268, saving model to weights-improvement-09-3.1127-bigger.hdf5\n","Epoch 10/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 2.9400 - acc: 0.2573\n","\n","Epoch 00010: loss improved from 3.11268 to 2.94001, saving model to weights-improvement-10-2.9400-bigger.hdf5\n","Epoch 11/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 2.6963 - acc: 0.3056\n","\n","Epoch 00011: loss improved from 2.94001 to 2.69635, saving model to weights-improvement-11-2.6963-bigger.hdf5\n","Epoch 12/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 2.4542 - acc: 0.3505\n","\n","Epoch 00012: loss improved from 2.69635 to 2.45419, saving model to weights-improvement-12-2.4542-bigger.hdf5\n","Epoch 13/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 2.2038 - acc: 0.4048\n","\n","Epoch 00013: loss improved from 2.45419 to 2.20378, saving model to weights-improvement-13-2.2038-bigger.hdf5\n","Epoch 14/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 1.9542 - acc: 0.4494\n","\n","Epoch 00014: loss improved from 2.20378 to 1.95421, saving model to weights-improvement-14-1.9542-bigger.hdf5\n","Epoch 15/200\n","6188/6188 [==============================] - 77s 12ms/step - loss: 1.7259 - acc: 0.5066\n","\n","Epoch 00015: loss improved from 1.95421 to 1.72590, saving model to weights-improvement-15-1.7259-bigger.hdf5\n","Epoch 16/200\n","6188/6188 [==============================] - 78s 13ms/step - loss: 1.5036 - acc: 0.5617\n","\n","Epoch 00016: loss improved from 1.72590 to 1.50357, saving model to weights-improvement-16-1.5036-bigger.hdf5\n","Epoch 17/200\n","6188/6188 [==============================] - 78s 13ms/step - loss: 1.2804 - acc: 0.6261\n","\n","Epoch 00017: loss improved from 1.50357 to 1.28038, saving model to weights-improvement-17-1.2804-bigger.hdf5\n","Epoch 18/200\n","6188/6188 [==============================] - 78s 13ms/step - loss: 1.0969 - acc: 0.6773\n","\n","Epoch 00018: loss improved from 1.28038 to 1.09691, saving model to weights-improvement-18-1.0969-bigger.hdf5\n","Epoch 19/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.9336 - acc: 0.7225\n","\n","Epoch 00019: loss improved from 1.09691 to 0.93358, saving model to weights-improvement-19-0.9336-bigger.hdf5\n","Epoch 20/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.7761 - acc: 0.7704\n","\n","Epoch 00020: loss improved from 0.93358 to 0.77607, saving model to weights-improvement-20-0.7761-bigger.hdf5\n","Epoch 21/200\n","6188/6188 [==============================] - 78s 13ms/step - loss: 0.6557 - acc: 0.8114\n","\n","Epoch 00021: loss improved from 0.77607 to 0.65568, saving model to weights-improvement-21-0.6557-bigger.hdf5\n","Epoch 22/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.5411 - acc: 0.8423\n","\n","Epoch 00022: loss improved from 0.65568 to 0.54114, saving model to weights-improvement-22-0.5411-bigger.hdf5\n","Epoch 23/200\n","6188/6188 [==============================] - 78s 13ms/step - loss: 0.4585 - acc: 0.8643\n","\n","Epoch 00023: loss improved from 0.54114 to 0.45854, saving model to weights-improvement-23-0.4585-bigger.hdf5\n","Epoch 24/200\n","6188/6188 [==============================] - 78s 13ms/step - loss: 0.3820 - acc: 0.8912\n","\n","Epoch 00024: loss improved from 0.45854 to 0.38196, saving model to weights-improvement-24-0.3820-bigger.hdf5\n","Epoch 25/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.3442 - acc: 0.9017\n","\n","Epoch 00025: loss improved from 0.38196 to 0.34425, saving model to weights-improvement-25-0.3442-bigger.hdf5\n","Epoch 26/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.2823 - acc: 0.9192\n","\n","Epoch 00026: loss improved from 0.34425 to 0.28229, saving model to weights-improvement-26-0.2823-bigger.hdf5\n","Epoch 27/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.2611 - acc: 0.9294\n","\n","Epoch 00027: loss improved from 0.28229 to 0.26109, saving model to weights-improvement-27-0.2611-bigger.hdf5\n","Epoch 28/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.2264 - acc: 0.9384\n","\n","Epoch 00028: loss improved from 0.26109 to 0.22636, saving model to weights-improvement-28-0.2264-bigger.hdf5\n","Epoch 29/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.2028 - acc: 0.9491\n","\n","Epoch 00029: loss improved from 0.22636 to 0.20275, saving model to weights-improvement-29-0.2028-bigger.hdf5\n","Epoch 30/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.1863 - acc: 0.9525\n","\n","Epoch 00030: loss improved from 0.20275 to 0.18626, saving model to weights-improvement-30-0.1863-bigger.hdf5\n","Epoch 31/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.1555 - acc: 0.9620\n","\n","Epoch 00031: loss improved from 0.18626 to 0.15554, saving model to weights-improvement-31-0.1555-bigger.hdf5\n","Epoch 32/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.1593 - acc: 0.9615\n","\n","Epoch 00032: loss did not improve from 0.15554\n","Epoch 33/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.1369 - acc: 0.9677\n","\n","Epoch 00033: loss improved from 0.15554 to 0.13685, saving model to weights-improvement-33-0.1369-bigger.hdf5\n","Epoch 34/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.1233 - acc: 0.9717\n","\n","Epoch 00034: loss improved from 0.13685 to 0.12329, saving model to weights-improvement-34-0.1233-bigger.hdf5\n","Epoch 35/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.1215 - acc: 0.9707\n","\n","Epoch 00035: loss improved from 0.12329 to 0.12153, saving model to weights-improvement-35-0.1215-bigger.hdf5\n","Epoch 36/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.1068 - acc: 0.9753\n","\n","Epoch 00036: loss improved from 0.12153 to 0.10680, saving model to weights-improvement-36-0.1068-bigger.hdf5\n","Epoch 37/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.0888 - acc: 0.9783\n","\n","Epoch 00037: loss improved from 0.10680 to 0.08878, saving model to weights-improvement-37-0.0888-bigger.hdf5\n","Epoch 38/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.0926 - acc: 0.9777\n","\n","Epoch 00038: loss did not improve from 0.08878\n","Epoch 39/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.0875 - acc: 0.9793\n","\n","Epoch 00039: loss improved from 0.08878 to 0.08750, saving model to weights-improvement-39-0.0875-bigger.hdf5\n","Epoch 40/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.0963 - acc: 0.9788\n","\n","Epoch 00040: loss did not improve from 0.08750\n","Epoch 41/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.0933 - acc: 0.9814\n","\n","Epoch 00041: loss did not improve from 0.08750\n","Epoch 42/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0776 - acc: 0.9809\n","\n","Epoch 00042: loss improved from 0.08750 to 0.07758, saving model to weights-improvement-42-0.0776-bigger.hdf5\n","Epoch 43/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0800 - acc: 0.9834\n","\n","Epoch 00043: loss did not improve from 0.07758\n","Epoch 44/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0701 - acc: 0.9848\n","\n","Epoch 00044: loss improved from 0.07758 to 0.07014, saving model to weights-improvement-44-0.0701-bigger.hdf5\n","Epoch 45/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0640 - acc: 0.9848\n","\n","Epoch 00045: loss improved from 0.07014 to 0.06398, saving model to weights-improvement-45-0.0640-bigger.hdf5\n","Epoch 46/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0624 - acc: 0.9859\n","\n","Epoch 00046: loss improved from 0.06398 to 0.06245, saving model to weights-improvement-46-0.0624-bigger.hdf5\n","Epoch 47/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0657 - acc: 0.9845\n","\n","Epoch 00047: loss did not improve from 0.06245\n","Epoch 48/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0618 - acc: 0.9867\n","\n","Epoch 00048: loss improved from 0.06245 to 0.06183, saving model to weights-improvement-48-0.0618-bigger.hdf5\n","Epoch 49/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0538 - acc: 0.9888\n","\n","Epoch 00049: loss improved from 0.06183 to 0.05380, saving model to weights-improvement-49-0.0538-bigger.hdf5\n","Epoch 50/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0655 - acc: 0.9864\n","\n","Epoch 00050: loss did not improve from 0.05380\n","Epoch 51/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0618 - acc: 0.9851\n","\n","Epoch 00051: loss did not improve from 0.05380\n","Epoch 52/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0513 - acc: 0.9879\n","\n","Epoch 00052: loss improved from 0.05380 to 0.05134, saving model to weights-improvement-52-0.0513-bigger.hdf5\n","Epoch 53/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0540 - acc: 0.9890\n","\n","Epoch 00053: loss did not improve from 0.05134\n","Epoch 54/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0515 - acc: 0.9885\n","\n","Epoch 00054: loss did not improve from 0.05134\n","Epoch 55/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0388 - acc: 0.9914\n","\n","Epoch 00055: loss improved from 0.05134 to 0.03880, saving model to weights-improvement-55-0.0388-bigger.hdf5\n","Epoch 56/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0526 - acc: 0.9872\n","\n","Epoch 00056: loss did not improve from 0.03880\n","Epoch 57/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0474 - acc: 0.9888\n","\n","Epoch 00057: loss did not improve from 0.03880\n","Epoch 58/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0480 - acc: 0.9887\n","\n","Epoch 00058: loss did not improve from 0.03880\n","Epoch 59/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0520 - acc: 0.9895\n","\n","Epoch 00059: loss did not improve from 0.03880\n","Epoch 60/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0474 - acc: 0.9906\n","\n","Epoch 00060: loss did not improve from 0.03880\n","Epoch 61/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0394 - acc: 0.9905\n","\n","Epoch 00061: loss did not improve from 0.03880\n","Epoch 62/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0357 - acc: 0.9924\n","\n","Epoch 00062: loss improved from 0.03880 to 0.03570, saving model to weights-improvement-62-0.0357-bigger.hdf5\n","Epoch 63/200\n","6188/6188 [==============================] - 81s 13ms/step - loss: 0.0679 - acc: 0.9866\n","\n","Epoch 00063: loss did not improve from 0.03570\n","Epoch 64/200\n","6188/6188 [==============================] - 80s 13ms/step - loss: 0.0284 - acc: 0.9947\n","\n","Epoch 00064: loss improved from 0.03570 to 0.02842, saving model to weights-improvement-64-0.0284-bigger.hdf5\n","Epoch 65/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.0488 - acc: 0.9885\n","\n","Epoch 00065: loss did not improve from 0.02842\n","Epoch 66/200\n","6188/6188 [==============================] - 79s 13ms/step - loss: 0.0467 - acc: 0.9898\n","\n","Epoch 00066: loss did not improve from 0.02842\n","Epoch 67/200\n","6188/6188 [==============================] - 77s 13ms/step - loss: 0.0488 - acc: 0.9913\n","\n","Epoch 00067: loss did not improve from 0.02842\n","Epoch 68/200\n","6188/6188 [==============================] - 76s 12ms/step - loss: 0.0398 - acc: 0.9918\n","\n","Epoch 00068: loss did not improve from 0.02842\n","Epoch 69/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0325 - acc: 0.9913\n","\n","Epoch 00069: loss did not improve from 0.02842\n","Epoch 70/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0403 - acc: 0.9911\n","\n","Epoch 00070: loss did not improve from 0.02842\n","Epoch 71/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0325 - acc: 0.9926\n","\n","Epoch 00071: loss did not improve from 0.02842\n","Epoch 72/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0381 - acc: 0.9906\n","\n","Epoch 00072: loss did not improve from 0.02842\n","Epoch 73/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0328 - acc: 0.9918\n","\n","Epoch 00073: loss did not improve from 0.02842\n","Epoch 74/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0362 - acc: 0.9918\n","\n","Epoch 00074: loss did not improve from 0.02842\n","Epoch 75/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0359 - acc: 0.9929\n","\n","Epoch 00075: loss did not improve from 0.02842\n","Epoch 76/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0396 - acc: 0.9914\n","\n","Epoch 00076: loss did not improve from 0.02842\n","Epoch 77/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0372 - acc: 0.9919\n","\n","Epoch 00077: loss did not improve from 0.02842\n","Epoch 78/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0267 - acc: 0.9945\n","\n","Epoch 00078: loss improved from 0.02842 to 0.02667, saving model to weights-improvement-78-0.0267-bigger.hdf5\n","Epoch 79/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0236 - acc: 0.9953\n","\n","Epoch 00079: loss improved from 0.02667 to 0.02365, saving model to weights-improvement-79-0.0236-bigger.hdf5\n","Epoch 80/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0267 - acc: 0.9935\n","\n","Epoch 00080: loss did not improve from 0.02365\n","Epoch 81/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0339 - acc: 0.9922\n","\n","Epoch 00081: loss did not improve from 0.02365\n","Epoch 82/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0204 - acc: 0.9958\n","\n","Epoch 00082: loss improved from 0.02365 to 0.02042, saving model to weights-improvement-82-0.0204-bigger.hdf5\n","Epoch 83/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0520 - acc: 0.9895\n","\n","Epoch 00083: loss did not improve from 0.02042\n","Epoch 84/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0373 - acc: 0.9906\n","\n","Epoch 00084: loss did not improve from 0.02042\n","Epoch 85/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0285 - acc: 0.9948\n","\n","Epoch 00085: loss did not improve from 0.02042\n","Epoch 86/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0234 - acc: 0.9948\n","\n","Epoch 00086: loss did not improve from 0.02042\n","Epoch 87/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0390 - acc: 0.9922\n","\n","Epoch 00087: loss did not improve from 0.02042\n","Epoch 88/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0282 - acc: 0.9939\n","\n","Epoch 00088: loss did not improve from 0.02042\n","Epoch 89/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0255 - acc: 0.9948\n","\n","Epoch 00089: loss did not improve from 0.02042\n","Epoch 90/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0356 - acc: 0.9934\n","\n","Epoch 00090: loss did not improve from 0.02042\n","Epoch 91/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0251 - acc: 0.9942\n","\n","Epoch 00091: loss did not improve from 0.02042\n","Epoch 92/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0189 - acc: 0.9958\n","\n","Epoch 00092: loss improved from 0.02042 to 0.01892, saving model to weights-improvement-92-0.0189-bigger.hdf5\n","Epoch 93/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0266 - acc: 0.9950\n","\n","Epoch 00093: loss did not improve from 0.01892\n","Epoch 94/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0385 - acc: 0.9924\n","\n","Epoch 00094: loss did not improve from 0.01892\n","Epoch 95/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0292 - acc: 0.9940\n","\n","Epoch 00095: loss did not improve from 0.01892\n","Epoch 96/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0274 - acc: 0.9934\n","\n","Epoch 00096: loss did not improve from 0.01892\n","Epoch 97/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0244 - acc: 0.9953\n","\n","Epoch 00097: loss did not improve from 0.01892\n","Epoch 98/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0563 - acc: 0.9900\n","\n","Epoch 00098: loss did not improve from 0.01892\n","Epoch 99/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0175 - acc: 0.9952\n","\n","Epoch 00099: loss improved from 0.01892 to 0.01751, saving model to weights-improvement-99-0.0175-bigger.hdf5\n","Epoch 100/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0431 - acc: 0.9911\n","\n","Epoch 00100: loss did not improve from 0.01751\n","Epoch 101/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0385 - acc: 0.9914\n","\n","Epoch 00101: loss did not improve from 0.01751\n","Epoch 102/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0273 - acc: 0.9939\n","\n","Epoch 00102: loss did not improve from 0.01751\n","Epoch 103/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0384 - acc: 0.9931\n","\n","Epoch 00103: loss did not improve from 0.01751\n","Epoch 104/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0213 - acc: 0.9956\n","\n","Epoch 00104: loss did not improve from 0.01751\n","Epoch 105/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0194 - acc: 0.9942\n","\n","Epoch 00105: loss did not improve from 0.01751\n","Epoch 106/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0243 - acc: 0.9942\n","\n","Epoch 00106: loss did not improve from 0.01751\n","Epoch 107/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0189 - acc: 0.9958\n","\n","Epoch 00107: loss did not improve from 0.01751\n","Epoch 108/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0362 - acc: 0.9927\n","\n","Epoch 00108: loss did not improve from 0.01751\n","Epoch 109/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0237 - acc: 0.9943\n","\n","Epoch 00109: loss did not improve from 0.01751\n","Epoch 110/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0194 - acc: 0.9948\n","\n","Epoch 00110: loss did not improve from 0.01751\n","Epoch 111/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0212 - acc: 0.9948\n","\n","Epoch 00111: loss did not improve from 0.01751\n","Epoch 112/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0217 - acc: 0.9960\n","\n","Epoch 00112: loss did not improve from 0.01751\n","Epoch 113/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0174 - acc: 0.9961\n","\n","Epoch 00113: loss improved from 0.01751 to 0.01743, saving model to weights-improvement-113-0.0174-bigger.hdf5\n","Epoch 114/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0212 - acc: 0.9952\n","\n","Epoch 00114: loss did not improve from 0.01743\n","Epoch 115/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0248 - acc: 0.9939\n","\n","Epoch 00115: loss did not improve from 0.01743\n","Epoch 116/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0181 - acc: 0.9960\n","\n","Epoch 00116: loss did not improve from 0.01743\n","Epoch 117/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0243 - acc: 0.9950\n","\n","Epoch 00117: loss did not improve from 0.01743\n","Epoch 118/200\n","6188/6188 [==============================] - 73s 12ms/step - loss: 0.0178 - acc: 0.9961\n","\n","Epoch 00118: loss did not improve from 0.01743\n","Epoch 119/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0235 - acc: 0.9958\n","\n","Epoch 00119: loss did not improve from 0.01743\n","Epoch 120/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0241 - acc: 0.9942\n","\n","Epoch 00120: loss did not improve from 0.01743\n","Epoch 121/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0309 - acc: 0.9935\n","\n","Epoch 00121: loss did not improve from 0.01743\n","Epoch 122/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0184 - acc: 0.9955\n","\n","Epoch 00122: loss did not improve from 0.01743\n","Epoch 123/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0188 - acc: 0.9955\n","\n","Epoch 00123: loss did not improve from 0.01743\n","Epoch 124/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0157 - acc: 0.9961\n","\n","Epoch 00124: loss improved from 0.01743 to 0.01566, saving model to weights-improvement-124-0.0157-bigger.hdf5\n","Epoch 125/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0227 - acc: 0.9956\n","\n","Epoch 00125: loss did not improve from 0.01566\n","Epoch 126/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0337 - acc: 0.9948\n","\n","Epoch 00126: loss did not improve from 0.01566\n","Epoch 127/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0323 - acc: 0.9940\n","\n","Epoch 00127: loss did not improve from 0.01566\n","Epoch 128/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0306 - acc: 0.9932\n","\n","Epoch 00128: loss did not improve from 0.01566\n","Epoch 129/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0337 - acc: 0.9916\n","\n","Epoch 00129: loss did not improve from 0.01566\n","Epoch 130/200\n","6188/6188 [==============================] - 74s 12ms/step - loss: 0.0251 - acc: 0.9931\n","\n","Epoch 00130: loss did not improve from 0.01566\n","Epoch 131/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0221 - acc: 0.9952\n","\n","Epoch 00131: loss did not improve from 0.01566\n","Epoch 132/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0228 - acc: 0.9942\n","\n","Epoch 00132: loss did not improve from 0.01566\n","Epoch 133/200\n","6188/6188 [==============================] - 76s 12ms/step - loss: 0.0212 - acc: 0.9955\n","\n","Epoch 00133: loss did not improve from 0.01566\n","Epoch 134/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0217 - acc: 0.9942\n","\n","Epoch 00134: loss did not improve from 0.01566\n","Epoch 135/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0211 - acc: 0.9947\n","\n","Epoch 00135: loss did not improve from 0.01566\n","Epoch 136/200\n","6188/6188 [==============================] - 75s 12ms/step - loss: 0.0288 - acc: 0.9942\n","\n","Epoch 00136: loss did not improve from 0.01566\n","Epoch 137/200\n","6188/6188 [==============================] - 76s 12ms/step - loss: 0.0219 - acc: 0.9940\n","\n","Epoch 00137: loss did not improve from 0.01566\n","Epoch 138/200\n","6188/6188 [==============================] - 76s 12ms/step - loss: 0.0176 - acc: 0.9952\n","\n","Epoch 00138: loss did not improve from 0.01566\n","Epoch 139/200\n","6188/6188 [==============================] - 76s 12ms/step - loss: 0.0184 - acc: 0.9953\n","\n","Epoch 00139: loss did not improve from 0.01566\n","Epoch 140/200\n","6188/6188 [==============================] - 76s 12ms/step - loss: 0.0157 - acc: 0.9958\n","\n","Epoch 00140: loss did not improve from 0.01566\n","Epoch 141/200\n","6188/6188 [==============================] - 76s 12ms/step - loss: 0.0124 - acc: 0.9966\n","\n","Epoch 00141: loss improved from 0.01566 to 0.01238, saving model to weights-improvement-141-0.0124-bigger.hdf5\n","Epoch 142/200\n","6188/6188 [==============================] - 76s 12ms/step - loss: 0.0119 - acc: 0.9961\n","\n","Epoch 00142: loss improved from 0.01238 to 0.01186, saving model to weights-improvement-142-0.0119-bigger.hdf5\n","Epoch 143/200\n","6188/6188 [==============================] - 77s 12ms/step - loss: 0.0228 - acc: 0.9953\n","\n","Epoch 00143: loss did not improve from 0.01186\n","Epoch 144/200\n","4960/6188 [=======================>......] - ETA: 15s - loss: 0.0209 - acc: 0.9954"],"name":"stdout"}]}]}